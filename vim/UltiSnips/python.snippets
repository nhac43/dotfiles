snippet class "create base class"
class ${1:}(${2:}):

	def __init__(self${3:}):
		${4:}
endsnippet

snippet def "create base function"
def ${1:}(${2:}):
	${3:}
endsnippet

snippet defs "create base method"
def ${1:}(self${2:}):
	${3:}
endsnippet

snippet main "create base main function"
def main():
	${1:}


if __name__ == "__main__":
	main()
endsnippet

snippet pars "create base argparser format"
parser = argparse.ArgumentParser()

parser.add_argument('--arg', type=str, default='')
parser.add_argument('--flag', action='store_true')
parser.add_argument('--choices', default=str, choices=[])
parser.add_argument('--multi', nargs='*')

args = parser.parse_args()

endsnippet

snippet seed "create set_seed function"
def seed_everything(seed=42):
	random.seed(seed)
	os.environ['PYTHONHASHSEED'] = str(seed)
	np.random.seed(seed)
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.backends.cudnn.deterministic = True

endsnippet

snippet importst "import ml libraries"
import os
import gc
import re
import glob
import argparse
import shutil
import ast
import sys
import copy
import json
import time
import math
import string
import pickle
import random
import joblib
import itertools
import warnings
warnings.filterwarnings("ignore")
from collection import defaultdict

endsnippet

snippet importds "import data science libraries"
import scipy as sp
import numpy as np
import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
from tqdm.auto import tqdm
from sklearn.metics import classification_report
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold

endsnippet

snippet importtorch "import torch libraries"
import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.nn.functional as F
from torch.optim import Adam, SGD, AdamW
from torch.utils.data import DataLoader, Dataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

endsnippet

snippet importhf "import hugging face libraries"
import tokenizers
import transformers
print(f"tokenizers.__version__: {tokenizers.__version__}")
print(f"transformers.__version__: {transformers.__version__}")
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
os.environ["TOKENIZERS_PARALLELISM"] = True

endsnippet

snippet === "create title block"
# ==================================================
# ${1:}
# ==================================================
endsnippet

snippet gfold "create group K fold"
kf = GroupKFold(n_splits=cfg.n_fold)
groups = ${1:}
for n, (train_index, val_index) in enumerate(kf.split(train, ${2:}, groups)):
	train.loc[val_index, 'fold'] = int(n)
train['fold'] = train['fold'].astype(int)
display(train.groupby('fold').size())
endsnippet

snippet sfold "create stratified K fold"
kf = StratifiedKFold(n_splits=cfg.n_fold, random_state=cfg.seed)
for n, (train_index, val_index) in enumerate(kf.split(train, ${1:})):
	train.loc[val_index, 'fold'] = int(n)
train['fold'] = train['fold'].astype(int)
display(train.groupby('fold').size())
endsnippet

snippet modelhf "create base model for transformers"
class CustomModel(nn.Module):
	def __init__(self, cfg, config_path=None, pretrained=False):
		super().__init__()
		self.cfg = cfg
		if config_path is None:
			self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)
		else:
			self.config = torch.load(config_path)
		if pretrained:
			self.model = AutoModel.from_pretrained(cfg.model, config=self.config)
		else:
			self.model = AutoModel(self.config)
		self.fc_dropout = nn.Dropout(cfg.fc_dropout)
		self.fc = nn.Linear(self.config.hidden_size, 1)
		self._init_weights(self.fc)
		
	def _init_weights(self, module):
		if isinstance(module, nn.Linear):
			module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
			if module.bias is not None:
				module.bias.data.zero_()
		elif isinstance(module, nn.Embedding):
			module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
			if module.padding_idx is not None:
				module.weight.data[module.padding_idx].zero_()
		elif isinstance(module, nn.LayerNorm):
			module.bias.data.zero_()
			module.weight.data.fill_(1.0)
		
	def feature(self, inputs):
		outputs = self.model(**inputs)
		last_hidden_states = outputs[0]
		return last_hidden_states

	def forward(self, inputs):
		feature = self.feature(inputs)
		output = self.fc(self.fc_dropout(feature))
		return output
endsnippet
